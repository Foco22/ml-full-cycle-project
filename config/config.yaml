# Google Cloud and Vertex AI Configuration
gcp:
  project_id: "your-gcp-project-id"
  region: "us-central1"
  bucket_name: "your-ml-bucket"
  service_account: "your-service-account@your-project.iam.gserviceaccount.com"

# Vertex AI Configuration
vertex_ai:
  staging_bucket: "gs://your-ml-bucket/staging"
  model_display_name: "ml-model"
  endpoint_display_name: "ml-endpoint"
  machine_type: "n1-standard-4"
  accelerator_type: "NVIDIA_TESLA_T4"
  accelerator_count: 1
  training:
    container_uri: "us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest"
    replica_count: 1

# Data Configuration
data:
  raw_data_path: "data/raw"
  processed_data_path: "data/processed"
  predictions_path: "data/predictions"
  train_test_split: 0.2
  random_state: 42

# SQL Database Configuration (from parameters.json)
sql:
  driver: "{ODBC Driver 17 for SQL Server}"
  server_prod: "gs-saw-datalake-prod.sql.azuresynapse.net"
  server_dev: "gs-saw-datalake-dev.sql.azuresynapse.net"
  database: "gssqlpooldatalake1"
  username: "usrazureaml"
  schema_prod: "bd_internas_dls_procesos_86"
  schema_dev: ""

# Model Configuration
model:
  algorithm: "random_forest"  # or "xgboost", "lightgbm", etc.
  hyperparameters:
    n_estimators: 100
    max_depth: 10
    random_state: 42
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"

# API Configuration
api:
  type: "cmf_chile"  # API type (cmf_chile, custom, etc.)
  currencies: ["usd", "eur", "uf"]  # For CMF Chile API
  currency_map:
    usd:
      api: "dolar"
      column: "usdclp_obs"
    eur:
      api: "euro"
      column: "eurclp_obs"
    uf:
      api: "uf"
      column: "ufclp"

# Pipeline Configuration
pipeline:
  mode: "incremental"  # Options: incremental, full, backfill
  backfill_days: 7  # Number of days to backfill if needed

  # Dataset configuration - customize per data source
  dataset:
    dataset_id: "data_ingestion"  # BigQuery dataset name
    table_id: "raw_data"  # BigQuery table name
    partition_field: "Fecha"  # Field to partition by (DATE or TIMESTAMP)
    cluster_fields: ["Fecha"]  # Fields to cluster by
    merge_key: "Fecha"  # Primary key for upsert operations

# Logging Configuration
logging:
  level: "INFO"
  log_dir: "logs"
  log_file: "ml_pipeline.log"
